{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./spam.csv', encoding='ISO-8859-1')\n",
    "\n",
    "data.dropna(axis=1, inplace=True)\n",
    "data['length'] = data['v2'].apply(lambda x: len(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "        tokens[i] = lemma_word\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "data['message'] = data['v2'].apply(lambda x: convert_to_lower(x))\n",
    "data['message'] = data['message'].apply(lambda x: remove_numbers(x))\n",
    "data['message'] = data['message'].apply(lambda x: remove_punctuation(x))\n",
    "data['message'] = data['message'].apply(lambda x: remove_stopwords(x))\n",
    "data['message'] = data['message'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "data['message'] = data['message'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data['length_after_cleaning'] = data['message'].apply(lambda x: len(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('spam',1)\n",
    "data = data.replace('ham',0)\n",
    "\n",
    "data = data.drop(['v2','length','length_after_cleaning'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into input features (X) and target variable (y)\n",
    "X = data['message'].values\n",
    "y = data['v1'].values\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature engineering using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "print(type(X_train_tfidf))\n",
    "\n",
    "# Define the pipeline with KMeansSMOTE\n",
    "pipeline = make_pipeline(KMeansSMOTE(random_state=42))\n",
    "\n",
    "# Apply SMOTE oversampling to the training data\n",
    "X_train_oversampled, y_train_oversampled = pipeline.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(X_train_tfidf.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Train the model\n",
    "data1=model.fit(X_train_oversampled, y_train_oversampled, epochs=15, batch_size=32, validation_split=0.3)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_label = label_encoder.inverse_transform([int(pred) for pred in y_pred])\n",
    "accuracy = accuracy_score(y_test, y_pred_label)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\"Phony �350 award - Todays Voda numbers ending XXXX are selected to receive a �350 award.\\\n",
    "                If you have a match please call 08712300220 quoting claim code 3100 standard rates app\",\n",
    "              \"Congratulations, you will now receive notifications. Follow this link to find out about our promotions and discounts\"]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    test_case = convert_to_lower(test_case)\n",
    "    test_case = remove_numbers(test_case)\n",
    "    test_case = remove_punctuation(test_case)\n",
    "    test_case = remove_stopwords(test_case)\n",
    "    test_case = remove_extra_white_spaces(test_case)\n",
    "    test_case = lemmatizing(test_case)\n",
    "\n",
    "    test_case=[test_case]\n",
    "    test_case_tfidf = tfidf_vectorizer.transform(test_case).toarray()\n",
    "\n",
    "    prediction = model.predict(test_case_tfidf)\n",
    "    prediction_label = label_encoder.inverse_transform([int(prediction)])\n",
    "    print(\"Predicted Label: Spam\" if prediction_label[0] else \"Predicted Label: Ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred_label)\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "\n",
    "ax[0].plot(data1.history['loss'], color='b', label=\"Training loss\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(data1.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
